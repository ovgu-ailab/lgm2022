---
layout: default
title: Reading 4
id: reading4
---


## Reading Assignment 4: Deep Generative Networks & Variational Autoencoders

[Chapter 20.9](http://www.deeplearningbook.org/contents/generative_models.html) on how to do back-propagation through random operations (5 pages)  
* reparametrization trick
* why this trick is not applicable to discrete stochastic operations

Also have a look at [Kingma & Welling's workshop slides at NIPS 2015 (slides 11-14)](https://web.archive.org/web/20170829170717/http://dpkingma.com/wordpress/wp-content/uploads/2015/12/talk_nips_workshop_2015.pdf)

[Chapter 20.10-20.10.2](http://www.deeplearningbook.org/contents/generative_models.html) on fully directed generative networks and differentiable generator nets (4 pages)
* structure and (at least one) method of training Sigmoid Belief Nets
* concept of differentiable generator nets
and how reparametrization trick is related

Finally, on VAEs:
- [Variational Autoencoders Explained](http://kvfrans.com/variational-autoencoders-explained/) - an easy intro to VAEs, but not very deep and lacking the probabilistic view
- [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) - VAEs from two sides: the deep learning perspective and the probabilistic model perspective
- [Chapter 20.10.3 on Variational Autoencoders](http://www.deeplearningbook.org/contents/generative_models.html) - to draw connections from VAEs to other models and to follow the bookâ€™s narrative

Additional resource:
- [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf) - a tutorial on VAEs, more explanatory/more text
